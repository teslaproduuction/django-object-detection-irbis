import os
import io
import numpy as np
from PIL import Image as I
import torch
import collections
from ast import literal_eval
from ultralytics import YOLO
from ultralytics.utils.plotting import Annotator

from django.views.generic.detail import DetailView
from django.views.generic import FormView
from django.contrib.auth.mixins import LoginRequiredMixin
from django.contrib import messages
from django.conf import settings
from django.shortcuts import render, get_object_or_404, redirect
from django.core.paginator import Paginator
from django.urls import reverse
from django.utils.translation import gettext_lazy as _
from django.http import JsonResponse, HttpResponseRedirect
from django.db.models import Q

from images.models import ImageFile
from .models import InferencedImage, ManualAnnotation
from .forms import InferencedImageForm, YoloModelForm, ManualAnnotationFormSetFactory
from modelmanager.models import MLModel


def calculate_iou(box1, box2):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç IoU (Intersection over Union) –º–µ–∂–¥—É –¥–≤—É–º—è bounding box'–∞–º–∏.
    –§–æ—Ä–º–∞—Ç box: {'x': center_x, 'y': center_y, 'width': width, 'height': height}
    """

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–∑ —Ü–µ–Ω—Ç—Ä–∞ –≤ —É–≥–ª—ã
    def center_to_corners(box):
        x_center, y_center, width, height = box['x'], box['y'], box['width'], box['height']
        x1 = x_center - width / 2
        y1 = y_center - height / 2
        x2 = x_center + width / 2
        y2 = y_center + height / 2
        return x1, y1, x2, y2

    x1_1, y1_1, x2_1, y2_1 = center_to_corners(box1)
    x1_2, y1_2, x2_2, y2_2 = center_to_corners(box2)

    # –í—ã—á–∏—Å–ª—è–µ–º –æ–±–ª–∞—Å—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
    x1_inter = max(x1_1, x1_2)
    y1_inter = max(y1_1, y1_2)
    x2_inter = min(x2_1, x2_2)
    y2_inter = min(y2_1, y2_2)

    # –ï—Å–ª–∏ –Ω–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
    if x1_inter >= x2_inter or y1_inter >= y2_inter:
        return 0.0

    # –ü–ª–æ—â–∞–¥—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
    intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)

    # –ü–ª–æ—â–∞–¥–∏ –±–æ–∫—Å–æ–≤
    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)

    # –ü–ª–æ—â–∞–¥—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    union = area1 + area2 - intersection

    return intersection / union if union > 0 else 0.0


def filter_automatic_detections_by_manual(automatic_detections, manual_detections, iou_threshold=0.1):
    """
    –§–∏–ª—å—Ç—Ä—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏, —É–¥–∞–ª—è—è —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –ø–µ—Ä–µ—Å–µ–∫–∞—é—Ç—Å—è —Å —Ä—É—á–Ω—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏.

    Args:
        automatic_detections: —Å–ø–∏—Å–æ–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–µ–∫—Ü–∏–π
        manual_detections: —Å–ø–∏—Å–æ–∫ —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        iou_threshold: –ø–æ—Ä–æ–≥ IoU –¥–ª—è —Å—á–∏—Ç–∞–Ω–∏—è –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.1)

    Returns:
        —Å–ø–∏—Å–æ–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–µ–∫—Ü–∏–π –±–µ–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π —Å —Ä—É—á–Ω—ã–º–∏
    """
    if not manual_detections:
        return automatic_detections

    filtered_detections = []

    for auto_detection in automatic_detections:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–µ—Ä–µ—Å–µ–∫–∞–µ—Ç—Å—è –ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è —Å –ª—é–±–æ–π —Ä—É—á–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π
        has_overlap = False

        for manual_detection in manual_detections:
            iou = calculate_iou(auto_detection, manual_detection)
            print(
                f"–ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ: –∞–≤—Ç–æ '{auto_detection.get('class', 'unknown')}' vs —Ä—É—á–Ω–∞—è '{manual_detection.get('class', 'unknown')}', IoU={iou:.3f}")

            if iou > iou_threshold:
                has_overlap = True
                print(f"‚úÖ –£–î–ê–õ–Ø–ï–ú –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –¥–µ—Ç–µ–∫—Ü–∏—é –∫–ª–∞—Å—Å–∞ '{auto_detection.get('class', 'unknown')}' "
                      f"–∏–∑-–∑–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è (IoU={iou:.3f}) —Å —Ä—É—á–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π –∫–ª–∞—Å—Å–∞ '{manual_detection.get('class', 'unknown')}'")
                break

        # –ï—Å–ª–∏ –Ω–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –¥–µ—Ç–µ–∫—Ü–∏—é
        if not has_overlap:
            print(
                f"‚úÖ –°–û–•–†–ê–ù–Ø–ï–ú –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –¥–µ—Ç–µ–∫—Ü–∏—é –∫–ª–∞—Å—Å–∞ '{auto_detection.get('class', 'unknown')}' - –Ω–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π")
            filtered_detections.append(auto_detection)

    return filtered_detections


def combine_detections_with_priority(automatic_detections, manual_detections, iou_threshold=0.1):
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏ —Ä—É—á–Ω—ã–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Ä—É—á–Ω—ã—Ö.

    Args:
        automatic_detections: —Å–ø–∏—Å–æ–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–µ–∫—Ü–∏–π
        manual_detections: —Å–ø–∏—Å–æ–∫ —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        iou_threshold: –ø–æ—Ä–æ–≥ IoU –¥–ª—è —Å—á–∏—Ç–∞–Ω–∏—è –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è

    Returns:
        –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    """
    print(f"üîÑ –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –î–ï–¢–ï–ö–¶–ò–ô:")
    print(f"  üìä –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–µ–∫—Ü–∏–π: {len(automatic_detections)}")
    print(f"  ‚úã –†—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {len(manual_detections)}")

    # –§–∏–ª—å—Ç—Ä—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏, —É–±–∏—Ä–∞—è –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å —Ä—É—á–Ω—ã–º–∏
    filtered_auto = filter_automatic_detections_by_manual(
        automatic_detections, manual_detections, iou_threshold
    )

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º: —Å–Ω–∞—á–∞–ª–∞ —Ä—É—á–Ω—ã–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç), –ø–æ—Ç–æ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ
    combined = manual_detections + filtered_auto

    print(f"  ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered_auto)}")
    print(f"  üéØ –ò—Ç–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ü–∏–π: {len(combined)}")

    return combined


class SaveAnnotationView(LoginRequiredMixin, FormView):
    """API –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —á–µ—Ä–µ–∑ AJAX –∑–∞–ø—Ä–æ—Å—ã."""

    def post(self, request, *args, **kwargs):
        if not request.is_ajax():
            return JsonResponse({'status': 'error', 'message': 'Only AJAX requests are allowed'}, status=400)

        image_id = request.POST.get('image_id')
        class_name = request.POST.get('class_name')
        x_center = float(request.POST.get('x_center', 0))
        y_center = float(request.POST.get('y_center', 0))
        width = float(request.POST.get('width', 0))
        height = float(request.POST.get('height', 0))

        if not image_id or not class_name:
            return JsonResponse({'status': 'error', 'message': 'Missing required data'}, status=400)

        try:
            image = ImageFile.objects.get(id=image_id)
            annotation = ManualAnnotation.objects.create(
                image=image,
                class_name=class_name,
                x_center=x_center,
                y_center=y_center,
                width=width,
                height=height,
                created_by=request.user,
                is_manual=True
            )

            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∫ –¥—É–±–ª–∏–∫–∞—Ç–∞–º
            self._apply_annotations_to_duplicates(image, annotation)

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å-–∑–∞–ø–∏—Å–∏ —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            self._update_existing_inferences(image)

            return JsonResponse({
                'status': 'success',
                'message': _('–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞'),
                'annotation_id': annotation.id
            })
        except Exception as e:
            return JsonResponse({'status': 'error', 'message': str(e)}, status=500)

    def _apply_annotations_to_duplicates(self, original_image, annotation):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º-–¥—É–±–ª–∏–∫–∞—Ç–∞–º."""
        if original_image.image_hash:
            duplicate_images = ImageFile.objects.filter(
                image_hash=original_image.image_hash
            ).exclude(id=original_image.id)

            for dup_img in duplicate_images:
                # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –¥—É–±–ª–∏–∫–∞—Ç–∞
                ManualAnnotation.objects.create(
                    image=dup_img,
                    class_name=annotation.class_name,
                    x_center=annotation.x_center,
                    y_center=annotation.y_center,
                    width=annotation.width,
                    height=annotation.height,
                    confidence=annotation.confidence,
                    created_by=annotation.created_by,
                    is_manual=True
                )

    def _update_existing_inferences(self, image):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å-–∑–∞–ø–∏—Å–∏ —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π."""
        manual_annotations = ManualAnnotation.objects.filter(image=image)
        manual_detection_info = [annotation.to_detection_format() for annotation in manual_annotations]

        existing_inferences = InferencedImage.objects.filter(orig_image=image)
        for inf_img in existing_inferences:
            is_custom_model = inf_img.custom_model is not None

            if inf_img.detection_info and is_custom_model:
                # –î–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
                combined_info = combine_detections_with_priority(
                    inf_img.detection_info, manual_detection_info
                )
                inf_img.detection_info = combined_info
                inf_img.save()
                print(f"‚úÖ SaveAnnotation: –û–±–Ω–æ–≤–ª–µ–Ω–∞ –∫–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º: {len(combined_info)} –¥–µ—Ç–µ–∫—Ü–∏–π")
            elif inf_img.detection_info and not is_custom_model:
                # –î–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö YOLO –º–æ–¥–µ–ª–µ–π –ù–ï –æ–±–Ω–æ–≤–ª—è–µ–º
                print(f"üö´ SaveAnnotation: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è YOLO –º–æ–¥–µ–ª—å - —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –ù–ï –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è")


class ManualAnnotationView(LoginRequiredMixin, FormView):
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."""

    template_name = 'detectobj/manual_annotation.html'
    form_class = ManualAnnotationFormSetFactory

    def get_success_url(self):
        return reverse('detectobj:detection_image_detail_url', kwargs={'pk': self.image.id})

    def dispatch(self, request, *args, **kwargs):
        # –ü–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ URL
        self.image = get_object_or_404(ImageFile, id=self.kwargs['pk'])
        return super().dispatch(request, *args, **kwargs)

    def get_form_kwargs(self):
        kwargs = super().get_form_kwargs()
        kwargs['image'] = self.image
        kwargs['user'] = self.request.user
        return kwargs

    def get_initial(self):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        initial = []

        # –ü–æ–ª—É—á–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        for annotation in ManualAnnotation.objects.filter(image=self.image):
            initial.append({
                'class_name': annotation.class_name,
                'x_center': annotation.x_center,
                'y_center': annotation.y_center,
                'width': annotation.width,
                'height': annotation.height,
            })

        # –ï—Å–ª–∏ –Ω–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∏—â–µ–º –≤ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
        if not initial and self.image.image_hash:
            duplicate_images = ImageFile.objects.filter(
                image_hash=self.image.image_hash
            ).exclude(id=self.image.id)

            for dup_img in duplicate_images:
                dup_annotations = ManualAnnotation.objects.filter(image=dup_img)
                if dup_annotations.exists():
                    messages.info(
                        self.request,
                        _("–ù–∞–π–¥–µ–Ω—ã –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å –∏–¥–µ–Ω—Ç–∏—á–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –¥—Ä—É–≥–æ–≥–æ –Ω–∞–±–æ—Ä–∞. –û–Ω–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.")
                    )
                    for annotation in dup_annotations:
                        initial.append({
                            'class_name': annotation.class_name,
                            'x_center': annotation.x_center,
                            'y_center': annotation.y_center,
                            'width': annotation.width,
                            'height': annotation.height,
                        })
                    break  # –ë–µ—Ä–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–∞

        return initial

    def form_valid(self, form):
        # Save the manual annotations
        form.save()
        messages.success(self.request, _("–ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã"))

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∫ –¥—É–±–ª–∏–∫–∞—Ç–∞–º
        self._apply_annotations_to_duplicates()

        # –¢–µ–ø–µ—Ä—å –æ–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ InferencedImage –∑–∞–ø–∏—Å–∏ —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        all_annotations = ManualAnnotation.objects.filter(image=self.image)

        if all_annotations.exists():
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Å–µ —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏
            manual_detection_info = [annotation.to_detection_format() for annotation in all_annotations]

            # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¢–û–õ–¨–ö–û –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            existing_inferences = InferencedImage.objects.filter(orig_image=self.image)
            updated_count = 0

            for inf_img in existing_inferences:
                is_custom_model = inf_img.custom_model is not None

                if is_custom_model:
                    # –î–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
                    # –ü–æ–ª—É—á–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏ (–±–µ–∑ —Å—Ç–∞—Ä—ã—Ö —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π)
                    original_detections = inf_img.detection_info or []

                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ (—É–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Ä—É—á–Ω—ã–µ)
                    automatic_detections = []
                    for item in original_detections:
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–æ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—é —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å —Ä—É—á–Ω—ã–º–∏
                        is_automatic = True
                        for manual_item in manual_detection_info:
                            if (abs(item.get('x', 0) - manual_item.get('x', 0)) < 0.01 and
                                    abs(item.get('y', 0) - manual_item.get('y', 0)) < 0.01 and
                                    abs(item.get('width', 0) - manual_item.get('width', 0)) < 0.01 and
                                    abs(item.get('height', 0) - manual_item.get('height', 0)) < 0.01):
                                is_automatic = False
                                break

                        if is_automatic:
                            automatic_detections.append(item)

                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
                    combined_info = combine_detections_with_priority(
                        automatic_detections, manual_detection_info
                    )

                    # –û–±–Ω–æ–≤–ª—è–µ–º detection_info
                    inf_img.detection_info = combined_info

                    # –ü–µ—Ä–µ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
                    try:
                        self._regenerate_inference_image(inf_img, combined_info)
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {e}")

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–ø–∏—Å—å
                    inf_img.save()
                    updated_count += 1

                    print(
                        f"‚úÖ –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å: –æ–±–Ω–æ–≤–ª–µ–Ω–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º ({len(automatic_detections)} –∞–≤—Ç–æ + {len(manual_detection_info)} —Ä—É—á–Ω—ã—Ö = {len(combined_info)})")
                else:
                    # –î–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö YOLO –º–æ–¥–µ–ª–µ–π –ù–ï –ø—Ä–∏–º–µ–Ω—è–µ–º —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                    print(f"üö´ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è YOLO –º–æ–¥–µ–ª—å: —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –ù–ï –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É –∏–Ω—Ñ–µ—Ä–µ–Ω—Å—É")

            if updated_count > 0:
                messages.info(self.request,
                              _("–ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –≤ %d –∑–∞–ø–∏—Å—è—Ö —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏") % updated_count)

        # Make sure to always redirect to the success URL
        return HttpResponseRedirect(self.get_success_url())

    def _regenerate_inference_image(self, inf_img, combined_detections):
        """–ü–µ—Ä–µ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏."""
        # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        inf_img_path = inf_img.inf_image_path
        if inf_img_path.startswith(settings.MEDIA_URL):
            inf_img_path = inf_img_path[len(settings.MEDIA_URL):]
        inf_img_full_path = os.path.join(settings.MEDIA_ROOT, inf_img_path)

        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if os.path.exists(inf_img_full_path):
            os.remove(inf_img_full_path)
            print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª: {inf_img_full_path}")

        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img_path = self.image.get_imagepath
        img = I.open(img_path)

        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏ —Ä—É—á–Ω—ã–µ
        manual_detections = [d for d in combined_detections if d.get('is_manual', False)]
        automatic_detections = [d for d in combined_detections if not d.get('is_manual', False)]

        if inf_img.custom_model:
            # –î–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏
            if manual_detections:
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏, —Ä–µ–Ω–¥–µ—Ä–∏–º –¢–û–õ–¨–ö–û –∏—Ö –Ω–∞ —á–∏—Å—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
                annotator = Annotator(np.array(img))  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ

                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∫—Ä–∞—Å–Ω—ã–º —Ü–≤–µ—Ç–æ–º
                for detection in manual_detections:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –ø–∏–∫—Å–µ–ª—å–Ω—ã–µ
                    img_width, img_height = img.size
                    x_center = detection.get('x', 0)
                    y_center = detection.get('y', 0)
                    width = detection.get('width', 0)
                    height = detection.get('height', 0)

                    x1 = int((x_center - width / 2) * img_width)
                    y1 = int((y_center - height / 2) * img_height)
                    x2 = int((x_center + width / 2) * img_width)
                    y2 = int((y_center + height / 2) * img_height)

                    # –ö—Ä–∞—Å–Ω—ã–π —Ü–≤–µ—Ç –¥–ª—è —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
                    color = (255, 0, 0)
                    annotator.box_label([x1, y1, x2, y2], detection.get('class', 'unknown'), color=color)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¢–û–õ–¨–ö–û —Å —Ä—É—á–Ω—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
                I.fromarray(annotator.result()).save(inf_img_full_path, format="JPEG")
                print(f"‚úÖ –ü–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è: –∫–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å —Å –¢–û–õ–¨–ö–û —Ä—É—á–Ω—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏")
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π, –∑–∞–ø—É—Å–∫–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
                model = YOLO(inf_img.custom_model.pth_filepath)
                results = model(img,
                                conf=float(inf_img.model_conf) if inf_img.model_conf else settings.MODEL_CONFIDENCE,
                                verbose=False)
                result = results[0]
                plotted_img = result.plot()
                I.fromarray(plotted_img).save(inf_img_full_path, format="JPEG")
                print(f"‚úÖ –ü–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è: –∫–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏")

        else:
            # –î–ª—è YOLO –º–æ–¥–µ–ª–∏ - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞ (–Ω–µ –¥–æ–ª–∂–Ω–æ –≤—ã–∑—ã–≤–∞—Ç—å—Å—è, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
            if inf_img.yolo_model:
                yolo_weightsdir = settings.YOLOV8_WEIGTHS_DIR
                model_path = os.path.join(yolo_weightsdir, inf_img.yolo_model)
                if os.path.exists(model_path):
                    model = YOLO(model_path)
                else:
                    model = YOLO(inf_img.yolo_model)

                results = model(img,
                                conf=float(inf_img.model_conf) if inf_img.model_conf else settings.MODEL_CONFIDENCE,
                                verbose=False)

                result = results[0]
                plotted_img = result.plot()
                I.fromarray(plotted_img).save(inf_img_full_path, format="JPEG")
                print(f"üìä –ü–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è YOLO –º–æ–¥–µ–ª—å")

    def _apply_annotations_to_duplicates(self):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Ç–µ–∫—É—â–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫ –µ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–∞–º."""
        if self.image.image_hash:
            duplicate_images = ImageFile.objects.filter(
                image_hash=self.image.image_hash
            ).exclude(id=self.image.id)

            if duplicate_images.exists():
                all_annotations = ManualAnnotation.objects.filter(image=self.image)

                for dup_img in duplicate_images:
                    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥—É–±–ª–∏–∫–∞—Ç–∞
                    ManualAnnotation.objects.filter(image=dup_img).delete()

                    # –ö–æ–ø–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                    for annotation in all_annotations:
                        ManualAnnotation.objects.create(
                            image=dup_img,
                            class_name=annotation.class_name,
                            x_center=annotation.x_center,
                            y_center=annotation.y_center,
                            width=annotation.width,
                            height=annotation.height,
                            confidence=annotation.confidence,
                            created_by=self.request.user,
                            is_manual=True
                        )

                messages.info(
                    self.request,
                    _("–ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Ç–∞–∫–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã –∫ %d –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –≤ –¥—Ä—É–≥–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö") % duplicate_images.count()
                )

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['image'] = self.image

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –≤—Å–µ—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        class_names = ManualAnnotation.objects.values_list('class_name', flat=True).distinct()
        context['class_names'] = list(class_names)

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã JavaScript
        img_width, img_height, *_ = self.image.get_imgshape()
        context['img_width'] = img_width
        context['img_height'] = img_height

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        if self.image.image_hash:
            duplicate_count = ImageFile.objects.filter(
                image_hash=self.image.image_hash
            ).exclude(id=self.image.id).count()

            if duplicate_count > 0:
                context['has_duplicates'] = True
                context['duplicate_count'] = duplicate_count

        return context


class InferenceImageDetectionView(LoginRequiredMixin, DetailView):
    model = ImageFile
    template_name = "detectobj/select_inference_image.html"

    def _get_all_annotations_for_image(self, img_qs):
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å –¥—É–±–ª–∏–∫–∞—Ç–æ–≤."""
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è —Å–∞–º–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        annotations = list(ManualAnnotation.objects.filter(image=img_qs))

        # –ï—Å–ª–∏ —É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –µ—Å—Ç—å —Ö—ç—à, –∏—â–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
        if img_qs.image_hash:
            duplicate_images = ImageFile.objects.filter(
                image_hash=img_qs.image_hash
            ).exclude(id=img_qs.id)

            for dup_img in duplicate_images:
                dup_annotations = ManualAnnotation.objects.filter(image=dup_img)
                if dup_annotations.exists():
                    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                    annotations.extend(list(dup_annotations))
                    break  # –ë–µ—Ä–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–∞

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º
        unique_annotations = []
        seen_coords = set()

        for annotation in annotations:
            coord_key = (
                round(annotation.x_center, 3),
                round(annotation.y_center, 3),
                round(annotation.width, 3),
                round(annotation.height, 3),
                annotation.class_name
            )
            if coord_key not in seen_coords:
                seen_coords.add(coord_key)
                unique_annotations.append(annotation)

        return unique_annotations

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        img_qs = self.get_object()
        imgset = img_qs.image_set
        images_qs = imgset.images.all()

        # For pagination GET request
        self.get_pagination(context, images_qs)

        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∏–Ω—Ñ–µ—Ä–µ–Ω—Å-–∫–∞—Ä—Ç–∏–Ω–∫—É
        if is_inf_img := InferencedImage.objects.filter(
                orig_image=img_qs
        ).exists():
            inf_img_qs = InferencedImage.objects.filter(orig_image=img_qs).first()
            context['inf_img_qs'] = inf_img_qs

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –∫–ª–∞—Å—Å–æ–≤, –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è
            if inf_img_qs.detection_info:
                classes_list = [item.get('class') for item in inf_img_qs.detection_info]
                context['results_counter'] = collections.Counter(classes_list)

        # –ü–æ–ª—É—á–∞–µ–º –í–°–ï —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (–≤–∫–ª—é—á–∞—è –¥—É–±–ª–∏–∫–∞—Ç—ã) –¢–û–õ–¨–ö–û –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        manual_annotations = self._get_all_annotations_for_image(img_qs)

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
        if 'inf_img_qs' in context:
            inf_img = context['inf_img_qs']
            is_custom_inference = inf_img.custom_model is not None

            if not is_custom_inference:
                # –î–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö YOLO –º–æ–¥–µ–ª–µ–π —Å–∫—Ä—ã–≤–∞–µ–º —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                manual_annotations = []
                print(f"üö´ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è YOLO –º–æ–¥–µ–ª—å: —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å–∫—Ä—ã—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")

        context['manual_annotations'] = manual_annotations

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
        if img_qs.image_hash:
            duplicate_images = ImageFile.objects.filter(
                image_hash=img_qs.image_hash
            ).exclude(id=img_qs.id)

            if duplicate_images.exists():
                context['has_duplicates'] = True
                context['duplicate_count'] = duplicate_images.count()
                context['duplicate_sets'] = [dup.image_set.name for dup in duplicate_images[:3]]  # –ü–µ—Ä–≤—ã–µ 3 –Ω–∞–±–æ—Ä–∞

        # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å, –¥–æ–±–∞–≤–ª—è–µ–º –∫ –Ω–µ–º—É —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if 'inf_img_qs' in context and manual_annotations:
            inf_img = context['inf_img_qs']
            detection_info = inf_img.detection_info or []

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –ª–∏ –∫–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å
            is_custom_inference = inf_img.custom_model is not None

            # –î–æ–±–∞–≤–ª—è–µ–º —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –¥–µ—Ç–µ–∫—Ü–∏–∏
            manual_detection_info = [annotation.to_detection_format() for annotation in manual_annotations]

            # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è —Ç–∏–ø–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–π
            for item in detection_info:
                if 'is_manual' not in item:
                    item['is_manual'] = item.get('confidence', 0) == 1.0  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ confidence=1.0 —ç—Ç–æ —Ä—É—á–Ω–∞—è

            for item in manual_detection_info:
                item['is_manual'] = True

            print(f"üéØ –ö–û–ù–¢–ï–ö–°–¢: –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–æ–≥–∏–∫—É –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
            print(f"  üìä –î–µ—Ç–µ–∫—Ü–∏–π –≤ inf_img: {len(detection_info)}")
            print(f"  ‚úã –†—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {len(manual_detection_info)}")
            print(f"  üéØ –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å –≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ: {is_custom_inference}")

            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
            if manual_detection_info:
                if is_custom_inference:
                    # –î–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
                    automatic_detections = [item for item in detection_info if not item.get('is_manual', False)]

                    combined_detection_info = combine_detections_with_priority(
                        automatic_detections, manual_detection_info
                    )
                    context['all_detection_info'] = combined_detection_info
                    print(f"‚úÖ –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å: —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ü–∏–π –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è: {len(combined_detection_info)}")
                else:
                    # –î–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö YOLO –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç–æ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                    combined_detection_info = detection_info + manual_detection_info
                    context['all_detection_info'] = combined_detection_info
                    print(f"‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è YOLO: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(combined_detection_info)}")

                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                all_classes = [item.get('class') for item in context['all_detection_info']]
                context['all_results_counter'] = collections.Counter(all_classes)
            else:
                context['all_detection_info'] = detection_info

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –¢–û–õ–¨–ö–û –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        context['manual_annotation_url'] = reverse('detectobj:manual_annotation_url', kwargs={'pk': img_qs.id})

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –∫–Ω–æ–ø–∫—É —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏
        show_manual_annotation = True
        if 'inf_img_qs' in context:
            inf_img = context['inf_img_qs']
            is_custom_inference = inf_img.custom_model is not None
            show_manual_annotation = is_custom_inference
            if not is_custom_inference:
                print(f"üö´ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è YOLO –º–æ–¥–µ–ª—å: –∫–Ω–æ–ø–∫–∞ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ —Å–∫—Ä—ã—Ç–∞")

        context['show_manual_annotation'] = show_manual_annotation

        context["img_qs"] = img_qs
        context["form1"] = YoloModelForm()
        context["form2"] = InferencedImageForm()
        return context

    def get_pagination(self, context, images_qs):
        paginator = Paginator(
            images_qs, settings.PAGINATE_DETECTION_IMAGES_NUM)
        page_number = self.request.GET.get('page')
        page_obj = paginator.get_page(page_number)
        context["is_paginated"] = (
                images_qs.count() > settings.PAGINATE_DETECTION_IMAGES_NUM
        )
        context["page_obj"] = page_obj

    def post(self, request, *args, **kwargs):
        img_qs = self.get_object()
        img_bytes = img_qs.image.read()
        img = I.open(io.BytesIO(img_bytes))

        # Initialize inf_img_qs to None to avoid UnboundLocalError later
        inf_img_qs = None

        # Get form data
        modelconf = self.request.POST.get("model_conf")
        modelconf = float(
            modelconf) if modelconf else settings.MODEL_CONFIDENCE
        custom_model_id = self.request.POST.get("custom_model")
        yolo_model_name = self.request.POST.get("yolo_model")

        # Yolov8 dirs
        yolo_weightsdir = settings.YOLOV8_WEIGTHS_DIR

        # Flag to track if we're using a custom model
        is_custom_model = False

        # Whether user selected a custom model for the detection task
        # An offline model will be used for detection provided user has
        # uploaded this model.
        if custom_model_id:
            detection_model = MLModel.objects.get(id=custom_model_id)
            model = YOLO(detection_model.pth_filepath)
            is_custom_model = True
            # Print class names for debugging
            print("Custom model class names:", model.names)

        # Whether user selected a yolo model for the detection task
        # Selected yolov8 model will be downloaded, and ready for object
        # detection task. YOLOv8 API will start working.
        elif yolo_model_name:
            model_path = os.path.join(yolo_weightsdir, yolo_model_name)
            # Download if not exists
            if not os.path.exists(model_path):
                model = YOLO(yolo_model_name)  # This will download the model
            else:
                model = YOLO(model_path)
            # Print class names for debugging
            print("YOLO model class names:", model.names)

        # If using custom model, pre-update class names before inference
        if is_custom_model:
            # Fix class names in model before running inference
            for idx in model.names:
                if model.names[idx].lower() in ['cat', 'sheep']:
                    model.names[idx] = 'irbis'
                    print(f"Updated class {idx} from 'cat'/'sheep' to 'irbis'")

        # Run inference
        results = model(img, conf=modelconf, verbose=False)

        # Process results
        results_list = []
        for r in results:
            # Update class names in results if using custom model
            if is_custom_model and hasattr(r, 'names'):
                for idx in r.names:
                    if r.names[idx].lower() in ['cat', 'sheep']:
                        r.names[idx] = 'irbis'

            detection_boxes = []
            for box in r.boxes:
                # Convert box data to dictionary
                b = box.xywhn[0].tolist()  # normalized xywh format
                cls_id = int(box.cls[0].item())
                conf = float(box.conf[0].item())

                # Get class name (will already be updated if custom model)
                class_name = model.names[cls_id]

                # Double check and update if still cat/sheep
                if is_custom_model and class_name.lower() in ['cat', 'giraffe']:
                    class_name = 'irbis'
                    model.names[cls_id] = 'irbis'

                detection_boxes.append({
                    "class": class_name,
                    "class_id": cls_id,
                    "confidence": conf,
                    "x": b[0],
                    "y": b[1],
                    "width": b[2],
                    "height": b[3]
                })
            results_list = detection_boxes

        # Get class occurrences
        classes_list = [item["class"] for item in results_list]
        results_counter = collections.Counter(classes_list)

        # –ü–æ–ª—É—á–∞–µ–º –í–°–ï —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (–≤–∫–ª—é—á–∞—è –¥—É–±–ª–∏–∫–∞—Ç—ã)
        manual_annotations = self._get_all_annotations_for_image(img_qs)

        # Create directory for inference images if it doesn't exist
        media_folder = settings.MEDIA_ROOT
        inferenced_img_dir = os.path.join(
            media_folder, "inferenced_image")
        if not os.path.exists(inferenced_img_dir):
            os.makedirs(inferenced_img_dir, exist_ok=True)

        # Set default values for variables used later
        img_filename = None
        inf_img = None

        if not results_list and not (manual_annotations and is_custom_model):
            messages.warning(
                request,
                _('–ú–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–≥–ª–∞ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å –æ–±—ä–µ–∫—Ç—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Ä—É—á–Ω—É—é —Ä–∞–∑–º–µ—Ç–∫—É.'))
        else:
            # Make sure class names are updated in results before plotting
            if is_custom_model:
                # Update class names in each result
                for r in results:
                    if hasattr(r, 'names'):
                        for idx in r.names:
                            if r.names[idx].lower() in ['cat', 'giraffe']:
                                r.names[idx] = 'irbis'

            # Save the annotated image with a unique name based on model type
            if custom_model_id:
                img_file_suffix = f"custom_{custom_model_id}"
            else:
                img_file_suffix = f"yolo_{os.path.splitext(yolo_model_name)[0]}"

            # Generate filename with model identifier to allow multiple detections of same image
            img_filename = f"{os.path.splitext(img_qs.name)[0]}_{img_file_suffix}{os.path.splitext(img_qs.name)[1]}"
            img_path = os.path.join(inferenced_img_dir, img_filename)

            # Create a new inference image record each time (don't use get_or_create)
            inf_img = InferencedImage(
                orig_image=img_qs,
                inf_image_path=f"{settings.MEDIA_URL}inferenced_image/{img_filename}",
            )

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –¢–û–õ–¨–ö–û –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            manual_detection_info = []
            if manual_annotations and is_custom_model:  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                manual_detection_info = [annotation.to_detection_format() for annotation in manual_annotations]
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –º–∞—Ä–∫–µ—Ä –¥–ª—è —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
                for item in manual_detection_info:
                    item['is_manual'] = True
                    item['confidence'] = 1.0  # –†—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤—Å–µ–≥–¥–∞ –∏–º–µ—é—Ç confidence 1.0

            # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–µ–∫—Ü–∏–π
            for item in results_list:
                item['is_manual'] = False

            print(f"üîç –û–ë–†–ê–ë–û–¢–ö–ê –î–ï–¢–ï–ö–¶–ò–ô:")
            print(f"  üìä –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–µ–∫—Ü–∏–π: {len(results_list)}")
            print(f"  ‚úã –†—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {len(manual_annotations) if manual_annotations else 0}")
            print(f"  üéØ –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å: {is_custom_model}")

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¢–û–õ–¨–ö–û –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            if manual_detection_info and is_custom_model:
                print("üéØ –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏...")
                combined_detection_info = combine_detections_with_priority(
                    results_list, manual_detection_info
                )
                inf_img.detection_info = combined_detection_info
                print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ç–µ–∫—Ü–∏–π: {len(combined_detection_info)}")
            elif manual_annotations and not is_custom_model:
                # –î–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö YOLO –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –ë–ï–ó —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                all_manual_detection_info = [annotation.to_detection_format() for annotation in manual_annotations]
                for item in all_manual_detection_info:
                    item['is_manual'] = True
                combined_info = results_list + all_manual_detection_info
                inf_img.detection_info = combined_info
                print(
                    f"üìä –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è YOLO –º–æ–¥–µ–ª—å: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ ({len(results_list)} –∞–≤—Ç–æ + {len(all_manual_detection_info)} —Ä—É—á–Ω—ã—Ö = {len(combined_info)})")
            else:
                inf_img.detection_info = results_list
                print(f"üìä –¢–æ–ª—å–∫–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏: {len(results_list)}")

            inf_img.model_conf = modelconf
            if custom_model_id:
                inf_img.custom_model = detection_model
            elif yolo_model_name:
                inf_img.yolo_model = yolo_model_name

            # –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
            if manual_annotations and is_custom_model:
                # –î–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å —Ä—É—á–Ω—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ —Ä–µ–Ω–¥–µ—Ä–∏–º –¢–û–õ–¨–ö–û —Ä—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –Ω–∞ —á–∏—Å—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
                all_manual_detection_info = [annotation.to_detection_format() for annotation in manual_annotations]
                annotator = Annotator(np.array(img))  # –ß–ò–°–¢–û–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–µ–∫—Ü–∏–π

                # Draw ONLY manual annotations in red
                for annotation in all_manual_detection_info:
                    # Get normalized coordinates
                    x_center = annotation.get('x', 0)
                    y_center = annotation.get('y', 0)
                    width = annotation.get('width', 0)
                    height = annotation.get('height', 0)

                    # Convert to pixel coordinates
                    img_width, img_height = img.size
                    x1 = int((x_center - width / 2) * img_width)
                    y1 = int((y_center - height / 2) * img_height)
                    x2 = int((x_center + width / 2) * img_width)
                    y2 = int((y_center + height / 2) * img_height)

                    # Set color (manual annotations in red)
                    color = (255, 0, 0)  # RGB for red

                    # Draw the box
                    annotator.box_label([x1, y1, x2, y2], annotation.get('class', 'unknown'), color=color)

                # Save the image with ONLY manual annotations
                I.fromarray(annotator.result()).save(img_path, format="JPEG")
                print(f"‚úÖ –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –ß–ò–°–¢–û–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¢–û–õ–¨–ö–û —Å —Ä—É—á–Ω—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏")

            elif results_list:
                # –î–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è YOLO –∏–ª–∏ –∫–∞—Å—Ç–æ–º–Ω–∞—è –±–µ–∑ —Ä—É—á–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π)
                result = results[0]
                plotted_img = result.plot()
                I.fromarray(plotted_img).save(img_path, format="JPEG")

                if is_custom_model:
                    print(f"‚úÖ –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏ (–Ω–µ—Ç —Ä—É—á–Ω—ã—Ö)")
                else:
                    print(f"üìä –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è YOLO: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏")

            elif manual_annotations and is_custom_model:
                # No model results, just manual annotations (—Ç–æ–ª—å–∫–æ –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)
                all_manual_detection_info = [annotation.to_detection_format() for annotation in manual_annotations]
                annotator = Annotator(np.array(img))

                # Draw manual annotations
                for annotation in all_manual_detection_info:
                    # Get normalized coordinates
                    x_center = annotation.get('x', 0)
                    y_center = annotation.get('y', 0)
                    width = annotation.get('width', 0)
                    height = annotation.get('height', 0)

                    # Convert to pixel coordinates
                    img_width, img_height = img.size
                    x1 = int((x_center - width / 2) * img_width)
                    y1 = int((y_center - height / 2) * img_height)
                    x2 = int((x_center + width / 2) * img_width)
                    y2 = int((y_center + height / 2) * img_height)

                    # Set color (manual annotations in red)
                    color = (255, 0, 0)  # RGB for red

                    # Draw the box
                    annotator.box_label([x1, y1, x2, y2], annotation.get('class', 'unknown'), color=color)

                # Save the image with manual annotations
                I.fromarray(annotator.result()).save(img_path, format="JPEG")
                print(f"‚úÖ –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Å —Ä—É—á–Ω—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏")

            # Save inference image
            inf_img.save()

            # Get the latest inference for display
            if inf_img:
                inf_img_qs = inf_img

        # Clear CUDA cache if using GPU
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # set image is_inferenced to true
        img_qs.is_inferenced = True
        img_qs.save()

        # Ready for rendering next image on same html page.
        imgset = img_qs.image_set
        images_qs = imgset.images.all()

        # For pagination POST request
        context = {}
        self.get_pagination(context, images_qs)

        context["img_qs"] = img_qs
        context["inferenced_img_dir"] = f"{settings.MEDIA_URL}inferenced_image/{img_filename}" if img_filename else None
        context["results_list"] = results_list
        context["results_counter"] = results_counter
        context["form1"] = YoloModelForm()
        context["form2"] = InferencedImageForm()

        # Add manual annotations to context
        context["manual_annotations"] = manual_annotations

        # Add link to manual annotation page –¢–û–õ–¨–ö–û –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        context['manual_annotation_url'] = reverse('detectobj:manual_annotation_url', kwargs={'pk': img_qs.id})

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –∫–Ω–æ–ø–∫—É —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏
        show_manual_annotation = is_custom_model  # –í POST –º–µ—Ç–æ–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–ª–∞–≥ –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        context['show_manual_annotation'] = show_manual_annotation

        if not is_custom_model:
            print(f"üö´ POST: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è YOLO –º–æ–¥–µ–ª—å - –∫–Ω–æ–ø–∫–∞ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ —Å–∫—Ä—ã—Ç–∞")

        # Add the latest inferenced image to the context if available
        if inf_img_qs:
            context["inf_img_qs"] = inf_img_qs

        return render(self.request, self.template_name, context)